# ğŸ‘„ Lipread - Video to Text Conversion using Deep Learning

[![TensorFlow](https://img.shields.io/badge/TensorFlow-2.x-FF6F00?style=flat&logo=tensorflow)](https://www.tensorflow.org/)
[![Python](https://img.shields.io/badge/Python-3.8+-3776AB?style=flat&logo=python)](https://www.python.org/)
[![OpenCV](https://img.shields.io/badge/OpenCV-4.x-5C3EE8?style=flat&logo=opencv)](https://opencv.org/)

A sophisticated deep learning application that performs **lip reading** - converting silent video footage of people speaking into text using advanced neural networks and computer vision techniques.

---

## ğŸŒŸ Features

- **3D Convolutional Neural Networks**: Utilizes 3D CNN layers to extract spatial and temporal features from video frames
- **Bidirectional LSTM Architecture**: Employs bidirectional LSTM layers for sequence-to-sequence learning
- **CTC Loss Function**: Implements Connectionist Temporal Classification for sequence alignment
- **Interactive GUI**: User-friendly Tkinter-based interface for easy video selection and processing
- **Real-time Processing**: Efficient video processing pipeline with frame extraction and normalization
- **Pre-trained Model**: Comes with trained model checkpoints ready for inference

---

## ğŸ¯ How It Works

The system uses a multi-stage pipeline:

1. **Video Loading**: Extracts frames from input video files
2. **Preprocessing**: 
   - Converts frames to grayscale
   - Crops to lip region (190:236, 80:220)
   - Normalizes pixel values using mean and standard deviation
3. **Feature Extraction**: 3D CNN layers capture spatial-temporal patterns
4. **Sequence Learning**: Bidirectional LSTM layers model temporal dependencies
5. **Decoding**: CTC decoder converts predictions to readable text

---

## ğŸ—ï¸ Architecture

```
Input Video (frames)
    â†“
Conv3D (128 filters) â†’ ReLU â†’ MaxPool3D
    â†“
Conv3D (256 filters) â†’ ReLU â†’ MaxPool3D
    â†“
Conv3D (75 filters) â†’ ReLU â†’ MaxPool3D
    â†“
TimeDistributed(Flatten)
    â†“
Bidirectional LSTM (128 units) â†’ Dropout(0.5)
    â†“
Bidirectional LSTM (128 units) â†’ Dropout(0.5)
    â†“
Dense (vocabulary_size + 1) â†’ Softmax
    â†“
CTC Decoder â†’ Output Text
```

---

## ğŸ“‹ Prerequisites

- Python 3.8 or higher
- TensorFlow 2.x
- OpenCV
- NumPy
- Tkinter (usually comes with Python)
- Keras

---

## ğŸš€ Installation

1. **Clone the repository**
   ```bash
   git clone <repository-url>
   cd CSE439
   ```

2. **Install required dependencies**
   ```bash
   pip install tensorflow opencv-python numpy matplotlib keras
   ```

3. **Verify model checkpoint**
   Ensure the pre-trained model exists at:
   ```
   ./models/checkpoint
   ```

---

## ğŸ’» Usage

### GUI Application

Run the graphical interface:

```bash
python gui.py
```

**Steps:**
1. Click **"Select Video File"** to choose a video (supports .mp4, .avi, .mkv, .wmv, .mpg)
2. Click **"Process Video"** to start lip reading
3. View the predicted text in the output window

### Jupyter Notebook

For experimentation and model training, use:

```bash
jupyter notebook main.ipynb
```

---

## ğŸ“ Project Structure

```
CSE439/
â”œâ”€â”€ gui.py              # Tkinter-based GUI application
â”œâ”€â”€ main.ipynb          # Jupyter notebook for training/experimentation
â”œâ”€â”€ models/
â”‚   â””â”€â”€ checkpoint      # Pre-trained model weights
â””â”€â”€ README.md           # Project documentation
```

---

## ğŸ”§ Configuration

### Vocabulary
The model supports the following character set:
```python
vocab = "abcdefghijklmnopqrstuvwxyz'?!123456789 "
```

### Video Processing
- **Input dimensions**: Full frame
- **Lip region crop**: [190:236, 80:220]
- **Color space**: Grayscale
- **Normalization**: Z-score normalization (mean=0, std=1)

---

## ğŸ§  Model Details

### Hyperparameters
- **Optimizer**: Adam
- **Loss Function**: CTC (Connectionist Temporal Classification)
- **Dropout Rate**: 0.5
- **LSTM Units**: 128 (Bidirectional)
- **CNN Filters**: 128 â†’ 256 â†’ 75

### Training
The model is trained to predict text sequences from video frames using CTC loss, which allows for alignment-free sequence learning.

---

## ğŸ“Š Technical Specifications

| Component | Specification |
|-----------|--------------|
| Framework | TensorFlow/Keras |
| Model Type | 3D CNN + Bidirectional LSTM |
| Input Format | Video files (.mpg, .mp4, .avi, .mkv, .wmv) |
| Output Format | Plain text |
| Decoding | CTC Greedy Decoder |
| Sequence Length | 75 frames |

---

## ğŸ“ Use Cases

- **Accessibility**: Assist hearing-impaired individuals
- **Silent Communication**: Decode speech in noisy environments
- **Security**: Surveillance and forensic analysis
- **Media**: Automatic subtitling for silent footage
- **Research**: Speech recognition and computer vision studies

---

## âš ï¸ Known Limitations

- Requires videos with clear lip movements
- Works best with frontal face views
- Fixed crop region may not suit all video formats
- Performance depends on video quality and lighting

---

## ğŸ”® Future Enhancements

- [ ] Multi-speaker support
- [ ] Real-time webcam processing
- [ ] Support for multiple languages
- [ ] Improved model architecture (Transformer-based)
- [ ] Data augmentation techniques
- [ ] Web-based interface
- [ ] API endpoint for integration

---

## ğŸ“š References

- Deep learning for lip reading
- Connectionist Temporal Classification (CTC)
- 3D Convolutional Neural Networks
- Bidirectional LSTM for sequence modeling

---

## ğŸ‘¥ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

---

## ğŸ“ License

This project is part of CSE439 coursework.

---

## ğŸ™ Acknowledgments

Special thanks to the CSE439 course instructors and the open-source community for making this project possible.

---

## ğŸ“§ Contact

For questions or feedback, please open an issue on the repository.

---

